---
layout: 'post'
title: 'aiacademy: 深度學習 Recurrnet Neural Network'
permalink: 'aiacademy/week9/recurrent-neural-network'
tags: aiacademy rnn
---


## 複習

### 神經網路概念複習

<iframe src="https://www.youtube.com/embed/wt3dF2Yv0m4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

### 一層神經網路矩陣表示法

<iframe src="https://www.youtube.com/embed/3E9020bgG-o" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

### 人工智慧解決問題的流程

<iframe src="https://www.youtube.com/embed/TpZAn5S6Dgc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## RNN

- 可以看我的筆記
   - [rnn1](https://yuting3656.github.io/yutingblog/dl-coursera-sequence-models/week1/rnn-sequence-models){:target="_back"}
   - [rnn2](https://yuting3656.github.io/yutingblog/dl-coursera-sequence-models/week1/rnn-sequence-models2){:target="_back"}


### RNN 概念介紹

<iframe src="https://www.youtube.com/embed/gLo35sB_luU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


### RNN Cell 運作方式

<iframe src="https://www.youtube.com/embed/GeX95Dyq0Ag" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


---
---
---
---
---

## RNN 應用

### RNN 應用之對話機器人

<iframe src="https://www.youtube.com/embed/U6Qczj1t3eg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

- 對話機器人
   - 每次輸入和輸出都不是固定長度!

### 對話機器人的變形應用

<iframe src="https://www.youtube.com/embed/l2ABj0TmHYs" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

- 應用

   - 翻譯
   - Video Captioning 生成影片敘述
   - 生成一段文字
   - 畫一半的圖完成它

- Andrej Karpathy

   - 李飛飛學生
   - 自動生成[數學教科書](http://karpathy.github.io/2015/05/21/rnn-effectiveness/){:target="_back"}

### 情意分析和 slot filing

<iframe src="https://www.youtube.com/embed/VlK5dJ5IgAU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

__Slot filing__

   ![Imgur](https://i.imgur.com/TW6hv7O.jpg)

### RNN 全壘打預測實例

<iframe src="https://www.youtube.com/embed/v5vD7I54GMU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

---
---
---
---
---

### RNN Cell 的運作方式

<iframe src="https://www.youtube.com/embed/eXqx2u9ELH0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

|![Imgur](https://i.imgur.com/mNkArxM.jpg)|![Imgur](https://i.imgur.com/8YiJ50I.jpg)|
|![Imgur](https://i.imgur.com/jPQDdnc.jpg)|![Imgur](https://i.imgur.com/e6hKY2V.jpg)|
|![Imgur](https://i.imgur.com/YD7uG8V.jpg)||

### RNN的應用類型

<iframe src="https://www.youtube.com/embed/xZjbjcDHcBM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

|![Imgur](https://i.imgur.com/fYzrYHA.jpg)|![Imgur](https://i.imgur.com/0i29ejJ.jpg)|
|![Imgur](https://i.imgur.com/D2ocCUR.jpg)|![Imgur](https://i.imgur.com/qAdJl3A.jpg)|

### Bidirectional RNN

<iframe src="https://www.youtube.com/embed/98yspofjfAo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

- 看看我的耳藝術天分!

   |![Imgur](https://i.imgur.com/SVWX63bl.jpg)|![Imgur](https://i.imgur.com/AtZFSef.jpg)|
   |![Imgur](https://i.imgur.com/ACpysYUl.jpg)||


---
---
---

## 可愛作業? LoL

### 簡單 RNN 作業

<iframe src="https://www.youtube.com/embed/hPefluokw2Q" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

|![Imgur](https://i.imgur.com/4EDcXz5l.jpg)|![Imgur](https://i.imgur.com/YI0yf7Vl.jpg)|

### Numpy 計算 RNN 作業

<iframe src="https://www.youtube.com/embed/cbX2DvpM02o" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

![Imgur](https://i.imgur.com/9lCA2Izl.jpg)

### 計算作業完整輸出

<iframe src="https://www.youtube.com/embed/7z1DaAN2dSc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


### 完成第一個 RNN

<iframe src="https://www.youtube.com/embed/__CblNUU95Q" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


---
---
---


### RNN 的學習法和問題

<iframe src="https://www.youtube.com/embed/hlcY2wfHOP4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<table>
  <tr>
     <td colspan="2"><img src="https://i.imgur.com/K9N7foxl.jpg"></td>
  </tr>
  <tr>
     <td><img src="https://i.imgur.com/wqILyt2l.jpg"/></td>
     <td><img src="https://i.imgur.com/ren19YRl.jpg"/></td>
  </tr>
  <tr>
     <td colspan="2"><img src="https://i.imgur.com/qYiWEEVl.jpg"/></td>
  </tr>
</table>


### LSTM 的 Gates


<iframe src="https://www.youtube.com/embed/wR26R7gFCiM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

|![Imgur](https://i.imgur.com/bvWzWryl.jpg)|![Imgur](https://i.imgur.com/7e7rtFQl.jpg)|![Imgur](https://i.imgur.com/xgtBdBpl.jpg)|

### LSTM的運作方式

<iframe src="https://www.youtube.com/embed/_pH5-x6F4KM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


### GRU

<iframe src="https://www.youtube.com/embed/ZSZ-FJsoMUE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


## 實作

#### 回顧複習 RNN

<iframe src="https://www.youtube.com/embed/JcKh5DE9Z68" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

- 設定 RNN 要輸出的大小

   - 先初始化 weight, weight shape = (n, m + n) = (3, 9)
      - n 代表要轉換的維度，m代表原本的feature大小

|![Imgur](https://i.imgur.com/17lvLRL.jpg)|
|![Imgur](https://i.imgur.com/qL9pAcA.jpg)|
|![Imgur](https://i.imgur.com/4GhvOw9.jpg)|
|![Imgur](https://i.imgur.com/V1DSnXQ.jpg)|
|![Imgur](https://i.imgur.com/qyJLuRx.jpg)|
|![Imgur](https://i.imgur.com/YFdA88H.jpg)|
|![Imgur](https://i.imgur.com/RkuJIeX.jpg)|
|![Imgur](https://i.imgur.com/N97NpnT.jpg)|
|![Imgur](https://i.imgur.com/JTVuYO2.jpg)|
|![Imgur](https://i.imgur.com/atrRuyf.jpg)|
|![Imgur](https://i.imgur.com/mRzw5C8.jpg)|
|![Imgur](https://i.imgur.com/cbXdz3E.jpg)|
|![Imgur](https://i.imgur.com/AGEF0HR.jpg)|
|![Imgur](https://i.imgur.com/OhSZorr.jpg)|
|![Imgur](https://i.imgur.com/CzeYfae.jpg)|

#### RNN 實作 MNIST

<iframe src="https://www.youtube.com/embed/Ebg5VSJsrXE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

1. 前置作業 import package

~~~python
import numpy as np
from pprint import pprint
import matplotlib.pyplot as plt
from sklearn.utils import shuffle 
import tensorflow as tf
~~~

2. Set hyperparameters

~~~python
learning_rate = 0.001
batch_size = 128
epochs = 10
~~~

3. Load data and preprocess

~~~python
(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()

print('Data shape: ', X_train[0].shape)
print('Label: ', y_train[2])
plt.figure(figsize=(6, 6))
plt.imshow(X_train[2], cmap='binary')
plt.show()

X_train = X_train / 255.
X_test = X_test / 255.
y_train = np.eye(10)[y_train[:]]
y_test = np.eye(10)[y_test[:]]

def batch_gen(X, y, batch_size):
   X, y = shuffle(X, y)
   batch_index = 0

   while batch_index < len(X):
      batch_X = X[batch_index : batch_index + batch_size]
      batch_y = y[batch_index : batch_index + batch_size]
      batch_index += batch_size
      yield batch_X, batch_y
~~~

4. Build the graph

~~~python
def Rnn_layer(inputs, units):
   BasicRNN_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=units)
   # init_stae = tf.zeros([tf.shape(inputs)[0], units])
   init_state = BasicRNN_cell.zero_state(tf.shape(inputs)[0], dtype=tf.float32) # shape = (batch, units)
   outputs, states = tf.nn.dynamic_rnn(BasicRNN_cell, inputs, initial_state=init_state)
   return outputs
~~~

~~~python
tf.reset_default_graph()

with tf.name_scope("inputs"):
    input_data = tf.placeholder(dtype=tf.float32, shape=[None, 28, 28], name="input_data")
    y_label = tf.placeholder(dtype=tf.float32, shape=[None, 10], name='label')

with tf.variabel_scope("RNN_layer"):
    outputs = RNN_layer(input_data, 32)

with tf.variable_scope("output_layer"):
    RNN_last_outputs = outputs[:,-1,:]  # outputs shape = (batch, timestep, feature)
    prediction = tf.layers.dense(inputs=RNN_last_outputs, units=10)

with tf.name_scope("loss"):
    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction,labels=y_label))

with tf.name_scope("optimizer"):
    opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)

with tf.name_scope("accuracy"):
    correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(y_label, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))    

init = tf.global_variables_initializer()
~~~

~~~python
# with tf.keras

tf.reset_default_graph()

with tf.name_scope("inputs"):
    input_data = tf.placeholder(dtype=tf.float32, shape=[None, 28, 28], name='input_data')
    y_label = tf.placeholder(dtype=tf.float32, shape=[None, 10], name='label')

with tf.variable_scope("RNN_layer"):
    rnn_out = tf.keras.layers.SimpleRNN(units=32)(input_data)

with tf.variable_scope("output_layer"):
    prediction = tf.layers.dense(inputs=rnn_out, units=10)

with tf.name_scope("loss"):
    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction,labels=y_label))

with tf.name_scope("optimizer"):
    opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)

with tf.name_scope("accuracy"):
    correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(y_label, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))    

init = tf.global_variables_initializer()
~~~

5. Tain the model

~~~python
sess = tf.Session()
sess.run(init)
~~~

~~~python
for epoch_index in range(epochs):
    loss_ls, acc_ls = [], []
    get_batch = batch_gen(X_train, y_train, batch_size)
    
    for batch_X, batch_y in get_batch:
        _,  batch_acc, batch_loss = sess.run([opt, accuracy, loss], feed_dict={input_data: batch_X, y_label:batch_y})
        loss_ls.append(batch_loss)
        acc_ls.append(batch_acc)

    print("Epoch ", epoch_index)
    print("Accuracy ", np.mean(acc_ls), "     Loss ", np.mean(loss_ls))
    print("__________________") 
~~~

~~~python
sess.close()
~~~